---
title: 
author: 
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 7
    fig_height: 7 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(data.table)
library(ggplot2)
library(ggrepel)
library(readtext) # for getting document and their info into a data frame 

#COURSE <- "~iqmr" # where we keep all the course data

# similar method: http://varianceexplained.org/r/trump-tweets/
```

## Constructing corpora from plain text files


```{r}
# create five column dataframe ("leg_all") of: 
# 1. file name, 2. title (abb.), 3. text, 4. year, 5. jurisdiction

# import folder of documents "text_leg" 
# note: must be stored in current working directory when running file
leg_all <- readtext("text_leg/", 
                docvarsfrom = "filenames", # file name becomes "doc_id"
                docvarnames = c("title", "year", "jurisdiction")) # title (abb.), year, and jurisdiction are extracted from file name

# remove file name ("doc_id") column and rearrange column order
leg_all <- select(leg_all, title, jurisdiction, year, text, -doc_id) 

# preview of newly created dataframe
head(leg_all) 

```



```{r}
# combine text into corpus document feature matrix ("corpdfm")
# this provides frequency counts of how many time each word appears in each document
corp <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp)

# previous of the newly created word frequency corpus
# note: "/" and #s are counted as words, but this doesn't matter for our purposes
head(corpdfm)
```

```{r}
# create dataframe that includes total feature (word) counts (total; EU; US)

# convert dfm to a temporary df in order to run manipulations
temp_df <- convert(corpdfm, to = "data.frame") 

# choose which jurisdictional groups to sum over
EU_rows <- c(1:6,12:15) # I manually set jurisdictional rows, but could also
US_rows <- c(7:11) # use grepl in future to sort based on signifiers in titles

# add 3 rows at bottom of word sums
temp_df <- convert(corpdfm, to = "data.frame") %>%
  bind_rows(mapply(sum, temp_df[, -1], USE.NAMES = T)) %>% # total sum
  bind_rows(mapply(sum, temp_df[EU_rows, -1], USE.NAMES = T)) %>% # EU row sum
  bind_rows(mapply(sum, temp_df[US_rows, -1], USE.NAMES = T)) # US row sum
  
# add col 1 labels for word count totals rows
temp_df[nrow(temp_df)-2, 1] <- "total"
temp_df[nrow(temp_df)-1, 1] <- "EU total"
temp_df[nrow(temp_df), 1] <- "US total"

# create rotated dataframe with total counts for later use
temp_df_flip <- t(temp_df)
temp_df_flip <- as.data.frame(temp_df_flip) 
colnames(temp_df_flip) <- as.character(unlist(temp_df_flip[1,])) # move row 1 to col header
temp_df_flip = temp_df_flip[-1, ] # remove duplicate row
temp_df_flip <- setDT(temp_df_flip, keep.rownames = T) # moves row names (words) into col 1
names(temp_df_flip)[1] <- "features" # rename col 1 (words) for later join

feature_counts_totals <- temp_df_flip %>% # save for later join
  select("features", "total", "EU total", "US total") # select rows to keep (as cols)

```


This section removes all words that occur fewer than once in either jurisdiction OR fewer than 20 times overall. Ensuring a word is present in both documents is necessary for running the naive bayes classifier further on. The total word requirement can also be used to set alternative thresholds. The full script will still run if you exclude this section (although the naive bayes classification will be messy.)

```{r}
# convert dfm to a temporary df in order to run manipulations
temp_df <- convert(corpdfm, to = "data.frame") 

# generates jurisdictional word totals
# (different from mapply() above b/c here we don't want the names included)
total_all <- mapply(sum, temp_df[, -1], USE.NAMES = F)
total_eu <- mapply(sum, temp_df[EU_rows, -1], USE.NAMES = F)
total_us <- mapply(sum, temp_df[US_rows, -1], USE.NAMES = F)

remove_words <- c() # create an empty list of columns to remove (later) 

# runs slow because you shouldn't run python in R... 
# REMINDER: if time, go back and see if there is a function for this!
for (i in 2:ncol(temp_df)) { # for each column in the data frame
   #if ((total_eu[i-1] < 1) | (total_us[i-1] < 1) | (total_all[i-1] < 20)) { # if word occurs less than once in either jurisdiction, or fewer than 10 times overall
   if (total_all[i-1] < 20) { # test just total
     remove_words <- append(remove_words, colnames(temp_df)[i]) # add word to the list of words to be removed
   } 
}

# now make new corpdfm WITHOUT the words that are infrequent
# (this process is repetitive, but was the best I could figure out
# due to challenges moving between dataframes and dfm objects)
corp2 <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp2, remove = remove_words)

# maybe come back and rename this corpdfm_clean
```


This section runs the Naive-Bayes classification model. 
- The "jurisdiction" is set as the "training label"
- The "prior" probability is 50/50, which means that the proportion of EU vs. US documents in the model does not impact the prediction (see `help(textmodel_nb)` for more information).
- In the summary output, estimated jurisdicational feature scores should be read in realtion to eachother by word. E.g., the word "no" is 0.001373/0.001323 times more likely to indicate an EU document than US document.

```{r}
leg_nb <- textmodel_nb(corpdfm, corpdfm$jurisdiction)
summary(leg_nb)
```

```{r}

# demo - DELETE
# leg_nb[["x"]] #1980 is first EU legislation
# test_df <- convert(leg_nb[["x"]], to = "data.frame") 
# head(test_df) # gives count of word by legislation (also above)

# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]])) 
feature_scores <- as.data.frame(feature_scores)

# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
  setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
  rename(features = rn) %>% # rename row name ("rn") col to "features"
  transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
  transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
  transform(log_ratio_EU_US = log(EU / US)) %>% # create col for log of EU/US score ratio
  arrange(ratio_EU_US) # sort features by ascending EU/US score ratio


# make a new dataframe to visualize the distribution of logged score ratios
# this can be deleted
feature_scores_ranked <- mutate(feature_scores, rank = order(log_ratio_EU_US))
# plot
ggplot(feature_scores_ranked, aes(x=log_ratio_EU_US, y = rank)) + geom_point() 

# this shows us that the dataset is biased towards EU words 



```

This graph shows that there isn't a bias in either direction - the 0 intercepts at almost the 50% point: at the word ranked # 537 out of a total of 1090 words (537/1090 = 0.493). Recall above that the threshold was set to exclude words that occur fewer than 20 times. When this threshold was set lower in previous tests (e.g. 1, 10), the (log) EU/US ratio scores tended to favor EU legislation, so this helps elimate the bias of document imbalance. 

```{r}
# add total feature counts - ISSUE: this step is messing up the ggplot so I moved it afterwards
feature_scores <- merge(feature_scores, feature_counts_totals, by = "features")
```

The following model only includes words that occur more than once in each jurisdiction's legislation, and at least 20 times overall. Therefore, words included in the word bundles that do not reach these criteria will not be plotted. However, they are available for viewing in the `feature_counts_totals` dataframe.

```{r}
testwords <- c("environment", "human", "health", "wellbeing", "climate", "cancer", "air", "health-protection", "health-care", "healthcare", "disease", "science", "lung", "prevention", "prevent", "prevented", "preventing") # see if "preventative" ever used in docs

# look up the counts for each of these words

econ_bundle <- c(
  "economy",
  "economic",
  "economics",
  "economical",
  "economically",
  "efficient",
  "efficiency",
  "efficiencies",
  "cost",
  "costs",
  "cost-effective",
  "cost-effectiveness",
  "effectiveness", # for "cost effectiveness" without hyphen --> confirm
  "benefit",
  "benefits",
  "trade",
  "trading",
  "market",
  "marginal",
  "business",
  "businesses")

health_bundle <- c(
  "breathing",
  "lung",
  "pulmonary",
  "population", # not sure if this belongs here - look at use
  "cancer",
  "disease",
  "diseases",
  "health",
  "health-protection",
  "health-care",
  "asthma",
  "human")

prevent_bundle <- c(
  "prevent",
  "prevented",
  "preventive",
  "prevention",
  "preventing",
  "precautions",
  "precautionary",
  "protect",
  "protection",
  "protective",
  "protecting")

eco_bundle <- c(
  "ecological",
  "nature",
  "ecosystems",
  "ecosystem",
  "forest",
  "forests",
  "agriculture",
  "agricultural",
  "vegetation",
  "trees",
  "acid", # for "acid deposition, re. acid rain
  "acidification", # same
  "water",
  "waters"
)

other_bundle <- c(
  "local",
  "incentives",
  "permit",
  "permits",
  "fiscal",
  "resources" 
)

#filter out the big list of word scores to just the words
#that we selected in "testwords"
test_bundle_plot <- arrange(filter(feature_scores, features %in% testwords),
                     log_ratio_EU_US)

econ_bundle_plot <- arrange(filter(feature_scores, features %in% econ_bundle),
                     log_ratio_EU_US)

health_bundle_plot <- arrange(filter(feature_scores, features %in% health_bundle),
                     log_ratio_EU_US)

prevent_bundle_plot <- arrange(filter(feature_scores, features %in% prevent_bundle),
                     log_ratio_EU_US)

eco_bundle_plot <- arrange(filter(feature_scores, features %in% eco_bundle),
                     log_ratio_EU_US)

other_bundle_plot <- arrange(filter(feature_scores, features %in% other_bundle),
                     log_ratio_EU_US)

test_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "test bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

econ_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "econ bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

health_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "health bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

prevent_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "prevent bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

eco_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "eco bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

other_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "other bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))





```


```{r}
testscores %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "test bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))
```




############# IGNORE ################

```{r, eval = F}
wfish <- textmodel_wordfish(corpdfm, dir = c(15, 4)) #update based on leg_all global environment
summary(wfish)
# beta is the "position" of the word (ideologically speaking)
# you don't know which direction beta goes,  
# (can guess that "rights" is R) so we figure this out next
# psi gives you frequency

```





```{r, eval = F}
preds  <- predict(wfish, interval = "confidence")

# grab the (only) internal element 'fit' and make it a data frame
preds <- as_tibble(preds$fit)
#bind the columns together and give an ordering
preds_dv <- mutate(bind_cols(docvars(corpdfm), preds),
                   leg_order = rank(fit)) # add a left to right ordering

 #gets rid of gray ggplot background
theme_set(theme_minimal())

#y is speaker order, color by party
ggplot(preds_dv, aes(x = fit, xmin = lwr, xmax = upr,
                     y = leg_order, col = jurisdiction)) + #can also do year
  geom_point() +
  geom_errorbarh(height = 0) +
  #scale_color_manual(values = c("blue", "red")) +
  scale_y_continuous(labels = preds_dv$doc_id,
                     minor_breaks = NULL,
                     breaks = preds_dv$leg_order) +
  labs(x = "Sorting", y = "Legislation") +
  ggtitle("Air Legislation",
          subtitle = "Sorted based on legislation text")
```

```{r, eval = F}
#position is the only thing that the model is looking at

#let's find out why feinstein is -2... 
#features means words; calling beta the score and psi 
# the offset (ie word frequency) for help
wscores <- tibble(word = wfish$features,
                  score = wfish$beta,
                  offset = wfish$psi)

#testwords <- c("environment", "pollution", "wellbeing", "human", "health", "climate", "air", "proportionality", "federal", "member", "cancer", "motor", "nature", "smog", "acid", "rain", "vegetation", "agriculture", "ozone", "luxembourg", "smoke", "prevention", "preventive", "congress", "economic", "fossil-fuel")


testwords <- c("environment", "human", "health", "wellbeing", "climate", "cancer", "air", "health-protection", "health-care", "healthcare", "disease", "science", "lung", "prevention", "prevent", "preventive")


#filter out the big list of word scores to just the words
#that we selected in "testwords"
testscores <- arrange(filter(wscores, word %in% testwords),
                      score)
testscores
# "score" brings position up or down



```

```{r, eval = F}
ggplot(wscores, aes(score, offset, label = word)) +
  geom_point(color = "grey", alpha = 0.2) +
  geom_text_repel(data = testscores, col = "black") +
  geom_point(data = testscores) +
  labs(x = "Word prediction score (rougly speaking: EU to left, US to the right)", y = "Offset parameter (~frequency)") +
  ggtitle("Estimated Word Positions for Selected Debate Vocabulary",
          subtitle = "Note: Offset parameter is roughly proportional to word frequency")
# can see the layers at the bottom for words only used 1 or 2 times
# high frequency words are more in the middle
# as frequencies get lower, they have more of a pulling effect
# aka the eiffel tower plot


#show me in this corpurs, every instance of the word air
#plus or minus 15 words (so i can see how used) and who said it
# kwic(corp, "air", window = 15)
# only two instances from boxer. 
# drag the console larger to see whole script


```

