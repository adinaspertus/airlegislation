---
title: Air Quality Legislation Comparative Analysis
author: Adina Spertus-Melhus
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 7
    fig_height: 7 
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(data.table)
library(ggplot2)
library(ggrepel)
library(readtext) # for getting document and their info into a data frame 

# similar method: http://varianceexplained.org/r/trump-tweets/
```

## Import text from legislation 

```{r}
# create five column dataframe ("leg_all") of: 
# 1. file name, 2. title (abb.), 3. text, 4. year, 5. jurisdiction

# import folder of documents "text_leg" 
# note: must be stored in current working directory when running file
leg_all <- readtext("text_leg/", 
                docvarsfrom = "filenames", # file name becomes "doc_id"
                docvarnames = c("title", "year", "jurisdiction")) # title (abb.), year, and jurisdiction are extracted from file name

# remove file name ("doc_id") column and rearrange column order
leg_all <- select(leg_all, title, jurisdiction, year, text, -doc_id) 

# preview of newly created dataframe
head(leg_all) 

```



```{r}
# combine text into corpus document feature matrix ("corpdfm")
# this provides frequency counts of how many time each word appears in each document
corp <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp)

# previous of the newly created word frequency corpus
# note: "/" and #s are counted as words, but this doesn't matter for our purposes
head(corpdfm)
```

## Prepare text data for analysis

```{r}
# create dataframe that includes total feature (word) counts (total; EU; US)

# convert dfm to a temporary df in order to run manipulations
temp_df <- convert(corpdfm, to = "data.frame") 

# choose which jurisdictional groups to sum over
EU_rows <- c(1:6,12:15) # I manually set jurisdictional rows, but could also
US_rows <- c(7:11) # use grepl in future to sort based on signifiers in titles

# add 3 rows at bottom of word sums
temp_df <- convert(corpdfm, to = "data.frame") %>%
  bind_rows(mapply(sum, temp_df[, -1], USE.NAMES = T)) %>% # total sum
  bind_rows(mapply(sum, temp_df[EU_rows, -1], USE.NAMES = T)) %>% # EU row sum
  bind_rows(mapply(sum, temp_df[US_rows, -1], USE.NAMES = T)) # US row sum
  
# add col 1 labels for word count totals rows
temp_df[nrow(temp_df)-2, 1] <- "total"
temp_df[nrow(temp_df)-1, 1] <- "EU total"
temp_df[nrow(temp_df), 1] <- "US total"

# create rotated dataframe with total counts for later use
temp_df_flip <- t(temp_df)
temp_df_flip <- as.data.frame(temp_df_flip) 
colnames(temp_df_flip) <- as.character(unlist(temp_df_flip[1,])) # move row 1 to col header
temp_df_flip = temp_df_flip[-1, ] # remove duplicate row
temp_df_flip <- setDT(temp_df_flip, keep.rownames = T) # moves row names (words) into col 1
names(temp_df_flip)[1] <- "features" # rename col 1 (words) for later join

feature_counts_totals <- temp_df_flip %>% # save for later join
  select("features", "total", "EU total", "US total") # select rows to keep (as cols)

```

This section removes all words that occur fewer than once in either jurisdiction OR fewer than 20 times overall. Ensuring a word is present in both documents is necessary for running the naive bayes classifier further on. The total word requirement can also be used to set alternative thresholds. The full script will still run if you exclude this section (although the naive bayes classification will be messy.)

```{r}
# convert dfm to a temporary df in order to run manipulations
temp_df <- convert(corpdfm, to = "data.frame") 

# generates jurisdictional word totals
# (different from mapply() above b/c here we don't want the names included)
total_all <- mapply(sum, temp_df[, -1], USE.NAMES = F)
total_eu <- mapply(sum, temp_df[EU_rows, -1], USE.NAMES = F)
total_us <- mapply(sum, temp_df[US_rows, -1], USE.NAMES = F)

remove_words <- c() # create an empty list of columns to remove (later) 

# runs slow because you shouldn't run python in R... 
# REMINDER: if time, go back and see if there is a function for this!
for (i in 2:ncol(temp_df)) { # for each column in the data frame
   if ((total_eu[i-1] < 1) | (total_us[i-1] < 1) | (total_all[i-1] < 20)) { # if word occurs less than once in either jurisdiction, or fewer than 20 times overall
   #if (total_all[i-1] < 20) { # test just total
     remove_words <- append(remove_words, colnames(temp_df)[i]) # add word to the list of words to be removed
   } 
}

# now make new corpdfm WITHOUT the words that are infrequent
# (this process is repetitive, but was the best I could figure out
# due to challenges moving between dataframes and dfm objects)
corp2 <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp2, remove = remove_words)

# maybe come back and rename this corpdfm_clean
```

## Text Analysis

This section runs the Naive-Bayes classification model. 

- The "jurisdiction" is set as the "training label"
- The "prior" probability is 50/50, which means that the proportion of EU vs. US documents in the model does not impact the prediction (see `help(textmodel_nb)` for more information).
- In the summary output, estimated jurisdicational feature scores should be read in realtion to eachother by word. E.g., the word "no" is 0.001373/0.001323 times more likely to indicate an EU document than US document.

```{r}
leg_nb <- textmodel_nb(corpdfm, corpdfm$jurisdiction)
summary(leg_nb)
```

```{r}

# demo - DELETE
# leg_nb[["x"]] #1980 is first EU legislation
# test_df <- convert(leg_nb[["x"]], to = "data.frame") 
# head(test_df) # gives count of word by legislation (also above)

# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]])) 
feature_scores <- as.data.frame(feature_scores)

# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
  setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
  rename(features = rn) %>% # rename row name ("rn") col to "features"
  transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
  transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
  transform(log_ratio_EU_US = log(EU / US)) %>% # create col for log of EU/US score ratio
  arrange(ratio_EU_US) # sort features by ascending EU/US score ratio


feature_scores_ranked <- mutate(feature_scores, rank = order(log_ratio_EU_US))
# plot
ggplot(feature_scores_ranked, aes(x=log_ratio_EU_US, y = rank)) + geom_point() 



```

This graph shows that there isn't a bias in either direction - the 0 intercepts at almost the 50% point: at the word ranked # 537 out of a total of 1090 words (537/1090 = 0.493). Recall above that the threshold was set to exclude words that occur fewer than 20 times. When this threshold was set lower in previous tests (e.g. 1, 10), the (log) EU/US ratio scores tended to favor EU legislation, so this helps elimate the bias of document imbalance. 

```{r}
# add total feature counts - ISSUE: this step is messing up the ggplot so I moved it afterwards
feature_scores <- merge(feature_scores, feature_counts_totals, by = "features")
```

## Analysis Visualization

The following model only includes words that occur more than once in each jurisdiction's legislation, and at least 20 times overall. (This value can be adjusted in line 121.) Therefore, words included in the word bundles that do not reach these criteria will not be plotted. However, they are available for viewing in the `feature_counts_totals` dataframe.

```{r}
# allocate word lists to plot tendency scores

econ_bundle <- c(
  "economy",
  "economic",
  "economics",
  "economical",
  "economically",
  "efficient",
  "efficiency",
  "efficiencies",
  "cost",
  "costs",
  "cost-effective",
  "cost-effectiveness",
  "effectiveness", # for "cost effectiveness" without hyphen --> confirm
  "benefit",
  "benefits",
  "trade",
  "trading",
  "market",
  "marginal",
  "business",
  "businesses")

health_bundle <- c(
  "breathing",
  "lung",
  "pulmonary",
  "population", # not sure if this belongs here - look at use
  "cancer",
  "disease",
  "diseases",
  "health",
  "health-protection",
  "health-care",
  "asthma",
  "human")

prevent_bundle <- c(
  "prevent",
  "prevented",
  "preventive",
  "prevention",
  "preventing",
  "precautions",
  "precautionary",
  "protect",
  "protection",
  "protective",
  "protecting")

eco_bundle <- c(
  "ecological",
  "nature",
  "ecosystems",
  "ecosystem",
  "forest",
  "forests",
  "agriculture",
  "agricultural",
  "vegetation",
  "trees",
  "acid", # for "acid deposition, re. acid rain
  "acidification", # same
  "water",
  "waters"
)

other_bundle <- c(
  "local",
  "incentives",
  "permit",
  "permits",
  "fiscal",
  "resources" 
)

# based on top scores for above word bundles
demowords <- c("vegetation", "precautions", "population", "ecosystems", "efficiency", "human", "health", "air", "costs", "marginal", "prevention", "trade", "resources", "cancer", "commerce", "fiscal", "business") 


#filter out the big list of word scores to just the words
#that we selected in the relevant bundle, and plot
demo_plot <- arrange(filter(feature_scores, features %in% demowords),
                     log_ratio_EU_US)

econ_bundle_plot <- arrange(filter(feature_scores, features %in% econ_bundle),
                     log_ratio_EU_US)

health_bundle_plot <- arrange(filter(feature_scores, features %in% health_bundle),
                     log_ratio_EU_US)

prevent_bundle_plot <- arrange(filter(feature_scores, features %in% prevent_bundle),
                     log_ratio_EU_US)

eco_bundle_plot <- arrange(filter(feature_scores, features %in% eco_bundle),
                     log_ratio_EU_US)

other_bundle_plot <- arrange(filter(feature_scores, features %in% other_bundle),
                     log_ratio_EU_US)

# demo bundle plot below

econ_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "econ bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

health_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "health bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

prevent_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "prevent bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

eco_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "eco bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))

other_bundle_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "other bundle", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral"))





```

Note: This graph will appear different than the one in the associated IASS blog post. This is because that graph was created with a frequency threshold of 10 instead of 20. You can change the threshold in line 121 to reproduce the results. 

```{r}
demo_plot %>%
  mutate(features = reorder(features, log_ratio_EU_US)) %>%
  ggplot(aes(features, log_ratio_EU_US, fill = log_ratio_EU_US < 0)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs (title = "Selection of Words with Highest Tendencies*", caption = "*Note: \"air\" and \"health\" are included for illustrative purposes despite lower log ratio scores.", x = "Words found in legislation", y = "EU / US log ratio") +
  scale_fill_manual(name = "Word is more suggestive of", labels = c("EU Legislation", "US Legislation"), values = c("light blue", "coral")) +
  theme(plot.caption = element_text(hjust = 0, face = "italic"))
```



