knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(data.table)
library(ggplot2)
library(ggrepel)
library(readtext) # for getting document and their info into a data frame
#COURSE <- "~iqmr" # where we keep all the course data
# similar method: http://varianceexplained.org/r/trump-tweets/
# create five column dataframe ("leg_all") of:
# 1. file name, 2. title (abb.), 3. text, 4. year, 5. jurisdiction
# import folder of documents "text_leg"
# note: must be stored in current working directory when running file
leg_all <- readtext("text_leg/",
docvarsfrom = "filenames", # file name becomes "doc_id"
docvarnames = c("title", "year", "jurisdiction")) # title (abb.), year, and jurisdiction are extracted from file name
# remove file name ("doc_id") column and rearrange column order
leg_all <- select(leg_all, title, jurisdiction, year, text, -doc_id)
# preview of newly created dataframe
head(leg_all)
# combine text into corpus dataframe ("corpdfm")
# this provides frequency counts of how many time each word appears in each document
corp <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp)
# previous of the newly created word frequency corpus
# note: "/" and #s are counted as words, but this doesn't matter for our purposes
head(corpdfm)
# run naive-bayes classification model
# "jurisdiction" is set as the "training label"
# "prior" probability is 50/50, meaning that the proportion of EU vs. US docs doesn't impact prediction (see "help(textmodel_nb)" for more information)
leg_nb <- textmodel_nb(corpdfm, corpdfm$jurisdiction)
summary(leg_nb)
# estimated feature scores should be read in relation to eachother by word
# e.g. the word "no" is 0.001373/0.001323 times more likely to indicate an EU than US doc
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
# plot
ggplot(fss_test, aes(x=log_ratio_EU_US, y = rank)) + geom_point() # not log
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
# plot
ggplot(feature_scores, aes(x=log_ratio_EU_US, y = rank)) + geom_point() # not log
# plot
ggplot(feature_scores, aes(x=log_ratio_EU_US, y = rank)) + geom_point()
# plot
ggplot(feature_scores, aes(x=log_ratio_EU_US)) + geom_point()
help(order)
feature_scores_ranked <- feature_scores %>%
mutate(rank = order(ratio_EU_US))
View(feature_scores)
feature_scores_ranked <- feature_scores %>%
mutate(rank = order.ratio_EU_US)
feature_scores_ranked <- feature_scores %>%
mutate(rank = order(ratio_EU_US))
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
View(feature_scores_ranked)
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
#arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(data.table)
library(ggplot2)
library(ggrepel)
library(readtext) # for getting document and their info into a data frame
#COURSE <- "~iqmr" # where we keep all the course data
# similar method: http://varianceexplained.org/r/trump-tweets/
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
#arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(data.table)
library(ggplot2)
library(ggrepel)
library(readtext) # for getting document and their info into a data frame
#COURSE <- "~iqmr" # where we keep all the course data
# similar method: http://varianceexplained.org/r/trump-tweets/
# create five column dataframe ("leg_all") of:
# 1. file name, 2. title (abb.), 3. text, 4. year, 5. jurisdiction
# import folder of documents "text_leg"
# note: must be stored in current working directory when running file
leg_all <- readtext("text_leg/",
docvarsfrom = "filenames", # file name becomes "doc_id"
docvarnames = c("title", "year", "jurisdiction")) # title (abb.), year, and jurisdiction are extracted from file name
# remove file name ("doc_id") column and rearrange column order
leg_all <- select(leg_all, title, jurisdiction, year, text, -doc_id)
# preview of newly created dataframe
head(leg_all)
# combine text into corpus dataframe ("corpdfm")
# this provides frequency counts of how many time each word appears in each document
corp <- corpus(leg_all, text_field = "text", docid_field = "title")
corpdfm <- dfm(corp)
# previous of the newly created word frequency corpus
# note: "/" and #s are counted as words, but this doesn't matter for our purposes
head(corpdfm)
# run naive-bayes classification model
# "jurisdiction" is set as the "training label"
# "prior" probability is 50/50, meaning that the proportion of EU vs. US docs doesn't impact prediction (see "help(textmodel_nb)" for more information)
leg_nb <- textmodel_nb(corpdfm, corpdfm$jurisdiction)
summary(leg_nb)
# estimated feature scores should be read in relation to eachother by word
# e.g. the word "no" is 0.001373/0.001323 times more likely to indicate an EU than US doc
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
#arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
mutate(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
mutate(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
mutate(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
#arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
#arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(feature_scores$ratio_EU_US))
# "Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()"
# demo
#leg_nb[["x"]] #1980 is first EU legislation
#test_df <- convert(leg_nb[["x"]], to = "data.frame")
#test_df # gives you count of word by legislation
# extract feature scores from naive bayes output to make a new dataframe
feature_scores <- t(as.data.frame(leg_nb[["param"]]))
feature_scores <- as.data.frame(feature_scores)
# organize the feature (i.e., word) score table
feature_scores <- feature_scores %>%
setDT(keep.rownames = TRUE) %>% # shift rownames to col 1
rename(features = rn) %>% # rename row name ("rn") col to "features"
transform(ratio_EU_US = EU / US) %>% # create col for EU/US score ratio
transform(ratio_US_EU = US / EU) %>% # create col for US/EU score ratio
transform(log_ratio_EU_US = log( EU / US)) %>% # create col for log of EU/US score ratio
arrange(ratio_EU_US) # sort features by ascending EU/US score ratio
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
# plot
ggplot(feature_scores_ranked, aes(x=log_ratio_EU_US, y = rank)) + geom_point()
#ggplot(f_scores_sort, aes(x=diff_EU_US, y = features)) + geom_point()
ggplot(feature_scores_ranked, aes(x=diff_EU_US, y = features)) + geom_point()
ggplot(feature_scores_ranked, aes(x=diff_EU_US, y = rank)) + geom_point()
ggplot(feature_scores_ranked, aes(x=ratio_EU_US, y = rank)) + geom_point()
# make a new dataframe to visualize the distribution of logged score ratios
# this can be deleted
feature_scores_ranked <- mutate(feature_scores, rank = order(ratio_EU_US))
# plot
ggplot(feature_scores_ranked, aes(x=log_ratio_EU_US, y = rank)) + geom_point()
